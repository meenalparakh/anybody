TRAINER:
  EVAL_TIMESTEPS_INTERVAL: 100000
  EVAL_TIMESTEPS: 300
  VIDEO_RENDER: false

MULTIENV:
  SEEDS: 0

FORCE_PROBLEM_SPEC_GEN: false
NUM_GOAL_RANDOMIZATIONS: 100  # initially 500
FIXED_GOAL: false
FIXED_INITIAL_STATE: false
FORCE_ROBO_LINK_USD_CONVERSION: true
FORCE_RECOMPUTE_LINK_INFO: true

AGENT:
  ADAPTIVE_REWARD_NORMALIZATION: false
  PPO:
    LEARNING_RATE_SCHEDULER: LinearWarmstartCosineLR
    ROLLOUTS: 128
    LEARNING_EPOCHS: 4
  LEARNING_STARTS: 0
  RANDOM_TIMESTEPS: 2000
  EXPERIMENT:
    WRITE_INTERVAL: 2000
    CHECKPOINT_INTERVAL: 25000

CURRICULUM:
  N_UPDATES: 50
  NUM_STEPS: 500000

REWARD:
  SPARSE_REWARD: false

MODEL:
  IMPLICIT_OBS: false
  POLICY:
    INITIAL_LOG_STD: 10.0  # should be 1 if actions are absolute (as then they are normalized)

ACTION:
  ABSOLUTE: false

OBSERVATION:
  JOINT_VALUE_ENCODER:
    TYPE: sinusoidal